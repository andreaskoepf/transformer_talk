## Transformer & Attention

http://jalammar.github.io/illustrated-transformer/


The fall of RNN / LSTM, Eugenio Culurciello, 2018  
https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0  
https://towardsdatascience.com/memory-attention-sequences-37456d271992  


Attention and Augmented Recurrent Neural Networks  
https://distill.pub/2016/augmented-rnns/ 


Transformer Blog-Post  
http://www.peterbloem.nl/blog/transformers 


How to code The Transformer in Pytorch  
https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec  
https://github.com/SamLynnEvans/Transformer


How Transformers Work  
https://towardsdatascience.com/transformers-141e32e69591


The Annotated Transformer  
http://nlp.seas.harvard.edu/2018/04/03/attention.html


Attention? Attention!  
https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html


Attention: Links to Papers at bottom from 2017  
https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129


Understanding BERT Transformer: Attention isnâ€™t all you need  
https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db


Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention  
https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1
https://github.com/jessevig/bertviz/tree/master/bertviz


Building the Transformer XL from Scratch  
https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/  
https://github.com/keitakurita/Practical_NLP_in_PyTorch/blob/master/deep_dives/transformer_xl_from_scratch.ipynb


Lecture covering attention  
https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-15-45-4343-a_tutorial_on.pdf


DistilBERT by Huggingface  
https://medium.com/huggingface/distilbert-8cf3380435b5


Facebook Blog Post about lasted advances: RoBERTa  
https://ai.facebook.com/blog/new-advances-in-natural-language-processing-to-better-connect-people/


BERT explained  
https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270


Facebook Blog: Making Transformers simpler and more efficient  
https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/

 
A review of BERT based models  
https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58


ImageNet NLP Moment  
http://ruder.io/nlp-imagenet/?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_121


NVIDIA large scale BERT training  
https://nv-adlr.github.io/MegatronLM


Self attention layer added to ResNet architecture (practical insights)  
https://forums.fast.ai/t/how-we-beat-the-5-epoch-imagewoof-leaderboard-score-some-new-techniques-to-consider/53453  



### Interesting related Papers

Energy and Policy Considerations for Deep Learning in NLP  
https://arxiv.org/pdf/1906.02243.pdf

Analysing Mathematical Reasoning Abilities of Neural Models, 2019-04-02, Google  
https://arxiv.org/abs/1904.01557


Self-Attention Generative Adversarial Networks  
https://arxiv.org/abs/1805.08318


Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction, 2018-11  
https://arxiv.org/abs/1808.03867


Hard Attention  
https://arxiv.org/abs/1908.07644 


Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI's gpt2 Transformer Model  
https://arxiv.org/abs/1908.08594
