# The Transformer - Attention Based Sequence Transduction

Slides of a 25 min deep-dive into the transformer sequence-to-sequence architecture 
([Attention Is All You Need, Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762). I gave this talk 
at the [Data Science Meetup MÃ¼nster](https://www.meetup.com/de-DE/Data-Science-Meetup-Muenster/) 
on September 19, 2019.

### Links to Resources

Fun:

- https://talktotransformer.com/
- https://transformer.huggingface.co/ (Write With Transformer)

Blogs:

- http://jalammar.github.io/
- http://nlp.seas.harvard.edu/2018/04/03/attention.html (The Annotated Transformer)
- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

PyTorch:

- https://huggingface.co/pytorch-transformers/
- https://github.com/pytorch/fairseq
- https://docs.fast.ai/text.models.html

Tensorflow:

- https://www.tensorflow.org/beta/tutorials/text/transformer
- https://github.com/google-research/bert
